{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30069e3f-076b-4ebd-9222-0c972f343b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일시</th>\n",
       "      <th>최고기온</th>\n",
       "      <th>최저기온</th>\n",
       "      <th>일교차</th>\n",
       "      <th>강수량</th>\n",
       "      <th>평균습도</th>\n",
       "      <th>평균풍속</th>\n",
       "      <th>일조합</th>\n",
       "      <th>일사합</th>\n",
       "      <th>일조율</th>\n",
       "      <th>평균기온</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-01-02</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>87.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960-01-03</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960-01-04</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-01-05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일시  최고기온  최저기온   일교차  강수량  평균습도  평균풍속  일조합  일사합  일조율  평균기온\n",
       "0  1960-01-01   2.2  -5.2   7.4  NaN  68.3   1.7  6.7  NaN  NaN  -1.6\n",
       "1  1960-01-02   1.2  -5.6   6.8  0.4  87.7   1.3  0.0  NaN  NaN  -1.9\n",
       "2  1960-01-03   8.7  -2.1  10.8  0.0  81.3   3.0  0.0  NaN  NaN   4.0\n",
       "3  1960-01-04  10.8   1.2   9.6  0.0  79.7   4.4  2.6  NaN  NaN   7.5\n",
       "4  1960-01-05   1.3  -8.2   9.5  NaN  44.0   5.1  8.2  NaN  NaN  -4.6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.fft\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "import random\n",
    "from prophet import Prophet\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "fix_seed = 9495\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(fix_seed)\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5ef5ab-db02-44aa-bd17-7cdce2f52be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>최고기온</th>\n",
       "      <th>최저기온</th>\n",
       "      <th>강수량</th>\n",
       "      <th>평균습도</th>\n",
       "      <th>일사일조차</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>68.3</td>\n",
       "      <td>-1.89</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-01-02</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>87.7</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960-01-03</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.3</td>\n",
       "      <td>4.81</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960-01-04</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.21</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-01-05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-3.39</td>\n",
       "      <td>-4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  최고기온  최저기온  강수량  평균습도  일사일조차    y\n",
       "0  1960-01-01   2.2  -5.2  0.4  68.3  -1.89 -1.6\n",
       "1  1960-01-02   1.2  -5.6  0.4  87.7   4.81 -1.9\n",
       "2  1960-01-03   8.7  -2.1  0.0  81.3   4.81  4.0\n",
       "3  1960-01-04  10.8   1.2  0.0  79.7   2.21  7.5\n",
       "4  1960-01-05   1.3  -8.2  0.0  44.0  -3.39 -4.6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['최고기온'].fillna(method='bfill', inplace=True)\n",
    "df_train['최저기온'].fillna(method='bfill', inplace=True)\n",
    "df_train['일교차'].fillna(method='bfill', inplace=True)\n",
    "df_train['강수량'].fillna(method='bfill', inplace=True)\n",
    "df_train['평균풍속'].fillna(method='bfill', inplace=True)\n",
    "df_train['일조합'].fillna(method='bfill', inplace=True)\n",
    "df_train['일사합'].fillna(method='bfill', inplace=True)\n",
    "df_train['일조율'].fillna(method='bfill', inplace=True)\n",
    "df_train['일사일조차'] = df_train['일사합'] - df_train['일조합']\n",
    "\n",
    "df_train.rename(columns={'일시':'date'}, inplace=True)\n",
    "df_train.rename(columns={'평균기온':'y'}, inplace=True)\n",
    "\n",
    "cols = ['date','최고기온','최저기온','강수량','평균습도','일사일조차','y']\n",
    "df_train = df_train[cols]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e989193c-558f-40be-85de-11accb371b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RSE(pred, true):\n",
    "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "def CORR(pred, true):\n",
    "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
    "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
    "    d += 1e-12\n",
    "    return 0.01*(u / d).mean(-1)\n",
    "\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred - true) ** 2)\n",
    "\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "\n",
    "def metric(pred, true):\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    rse = RSE(pred, true)\n",
    "    corr = CORR(pred, true)\n",
    "\n",
    "    return mae, mse, rmse, mape, mspe, rse, corr\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda31ad2-d264-4eb5-84c7-0d72f687717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, scheduler, epoch, learning_rate, lradj, printout=True):\n",
    "    if lradj == 'type1':\n",
    "        lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif lradj == 'type3':\n",
    "        lr_adjust = {epoch: learning_rate if epoch < 3 else learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif lradj == 'constant':\n",
    "        lr_adjust = {epoch: learning_rate}\n",
    "    elif lradj == '3':\n",
    "        lr_adjust = {epoch: learning_rate if epoch < 10 else learning_rate*0.1}\n",
    "    elif lradj == '4':\n",
    "        lr_adjust = {epoch: learning_rate if epoch < 15 else learning_rate*0.1}\n",
    "    elif lradj == '5':\n",
    "        lr_adjust = {epoch: learning_rate if epoch < 25 else learning_rate*0.1}\n",
    "    elif lradj == '6':\n",
    "        lr_adjust = {epoch: learning_rate if epoch < 5 else learning_rate*0.1}  \n",
    "    elif lradj == 'TST':\n",
    "        lr_adjust = {epoch: scheduler.get_last_lr()[0]}\n",
    "    \n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if printout: print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visual(true, preds=None, name='./pic/test.pdf'):\n",
    "    \"\"\"\n",
    "    Results visualization\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
    "    if preds is not None:\n",
    "        plt.plot(preds, label='Prediction', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, bbox_inches='tight')\n",
    "\n",
    "def test_params_flop(model,x_shape):\n",
    "    \"\"\"\n",
    "    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()\n",
    "    \"\"\"\n",
    "    model_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        model_params += parameter.numel()\n",
    "        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))\n",
    "    from ptflops import get_model_complexity_info    \n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)\n",
    "        # print('Flops:' + flops)\n",
    "        # print('Params:' + params)\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc01880a-f524-46ed-a690-87f14be7ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, df ,flag='train', size=None, target='y', scale=True, timeenc=0, freq='d'):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.df = df\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = self.df.copy()\n",
    "\n",
    "        '''\n",
    "        df_raw.columns: ['date', ...(other features), target feature]\n",
    "        '''\n",
    "        print(self.target)\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "        num_train = int(len(df_raw) * 0.9)\n",
    "        num_test = int(len(df_raw) * 0.05)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        \n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            # print(self.scaler.mean_)\n",
    "            # exit()\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n",
    "\n",
    "class Dataset_Pred(Dataset):\n",
    "    def __init__(self, df, flag='pred', size=None, target='y', scale=True, inverse=False, timeenc=0, freq='d', cols=None):\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['pred']\n",
    "\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.inverse = inverse\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.cols = cols\n",
    "        self.df = df\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = self.df.copy()\n",
    "        '''\n",
    "        df_raw.columns: ['date', ...(other features), target feature]\n",
    "        '''\n",
    "        if self.cols:\n",
    "            cols = self.cols.copy()\n",
    "            cols.remove(self.target)\n",
    "        else:\n",
    "            cols = list(df_raw.columns)\n",
    "            cols.remove(self.target)\n",
    "            cols.remove('date')\n",
    "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "        border1 = len(df_raw) - self.seq_len\n",
    "        border2 = len(df_raw)\n",
    "\n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "        if self.scale:\n",
    "            self.scaler.fit(df_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        tmp_stamp = df_raw[['date']][border1:border2]\n",
    "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
    "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)\n",
    "\n",
    "        df_stamp = pd.DataFrame(columns=['date'])\n",
    "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        if self.inverse:\n",
    "            self.data_y = df_data.values[border1:border2]\n",
    "        else:\n",
    "            self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        if self.inverse:\n",
    "            seq_y = self.data_x[r_begin:r_begin + self.label_len]\n",
    "        else:\n",
    "            seq_y = self.data_y[r_begin:r_begin + self.label_len]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7410580-dd82-46c3-a28e-0636a84edf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_provider(batch_size, freq , seq_len, label_len, pred_len, target, num_workers, df ,flag):\n",
    "    Data = Dataset_Custom\n",
    "    timeenc = 1\n",
    "    shuffle_flag = True\n",
    "    drop_last = True\n",
    "    data = df.copy()\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        batch_size = batch_size\n",
    "        freq = freq\n",
    "    elif flag == 'pred':\n",
    "        shuffle_flag = False\n",
    "        drop_last = False\n",
    "        batch_size = 1\n",
    "        Data = Dataset_Pred\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = batch_size\n",
    "\n",
    "    data_set = Data(\n",
    "        df = data,\n",
    "        flag=flag,\n",
    "        size=[seq_len, label_len, pred_len],\n",
    "        target=target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq\n",
    "    )\n",
    "    print(flag, len(data_set))\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers= num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd47b2d5-2d02-49fa-b6a0-d8657dae979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        self.mean = None\n",
    "        self.stdev = None\n",
    "        self.last = None\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, patch_len, stride, padding_patch):\n",
    "        super(Backbone, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len = seq_len\n",
    "        self.pred_len = pred_len = pred_len\n",
    "\n",
    "        # Patching\n",
    "        self.patch_len = patch_len \n",
    "        self.stride = stride \n",
    "        self.patch_num = patch_num = int((seq_len - patch_len) / stride + 1)\n",
    "        self.padding_patch = padding_patch\n",
    "        if self.padding_patch == 'end':  # can be modified to general case\n",
    "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
    "            self.patch_num = patch_num = patch_num + 1\n",
    "\n",
    "        # 1\n",
    "        d_model = patch_len * patch_len\n",
    "        self.embed = nn.Linear(patch_len, d_model)\n",
    "        self.dropout_embed = nn.Dropout(0.3)\n",
    "\n",
    "        # 2\n",
    "        # self.lin_res = nn.Linear(seq_len, pred_len) # direct res, seems bad\n",
    "        self.lin_res = nn.Linear(patch_num * d_model, pred_len)\n",
    "        self.dropout_res = nn.Dropout(0.3)\n",
    "\n",
    "        # 3.1\n",
    "        self.depth_conv = nn.Conv1d(patch_num, patch_num, kernel_size=patch_len, stride=patch_len, groups=patch_num)\n",
    "        self.depth_activation = nn.GELU()\n",
    "        self.depth_norm = nn.BatchNorm1d(patch_num)\n",
    "        self.depth_res = nn.Linear(d_model, patch_len)\n",
    "        # 3.2\n",
    "        # self.point_conv = nn.Conv1d(patch_len,patch_len,kernel_size=1, stride=1)\n",
    "        # self.point_activation = nn.GELU()\n",
    "        # self.point_norm = nn.BatchNorm1d(patch_len)\n",
    "        self.point_conv = nn.Conv1d(patch_num, patch_num, kernel_size=1, stride=1)\n",
    "        self.point_activation = nn.GELU()\n",
    "        self.point_norm = nn.BatchNorm1d(patch_num)\n",
    "        # 4\n",
    "        self.mlp = Mlp(patch_len * patch_num, pred_len * 2, pred_len)\n",
    "\n",
    "    def forward(self, x): # B, L, D -> B, H, D\n",
    "        B, _, D = x.shape\n",
    "        L = self.patch_num\n",
    "        P = self.patch_len\n",
    "\n",
    "        # z_res = self.lin_res(x.permute(0, 2, 1)) # B, L, D -> B, H, D\n",
    "        # z_res = self.dropout_res(z_res)\n",
    "\n",
    "        # 1\n",
    "        if self.padding_patch == 'end':\n",
    "            z = self.padding_patch_layer(x.permute(0, 2, 1))  # B, L, D -> B, D, L -> B, D, L\n",
    "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride) # B, D, L, P\n",
    "        z = z.reshape(B * D, L, P, 1).squeeze(-1)\n",
    "        z = self.embed(z) # B * D, L, P -> # B * D, L, d\n",
    "        z = self.dropout_embed(z)\n",
    "\n",
    "        # 2\n",
    "        z_res = self.lin_res(z.reshape(B, D, -1)) # B * D, L, d -> B, D, L * d -> B, D, H\n",
    "        z_res = self.dropout_res(z_res)\n",
    "\n",
    "        # 3.1\n",
    "        res = self.depth_res(z) # B * D, L, d -> B * D, L, P\n",
    "        z_depth = self.depth_conv(z) # B * D, L, d -> B * D, L, P\n",
    "        z_depth = self.depth_activation(z_depth)\n",
    "        z_depth = self.depth_norm(z_depth)\n",
    "        z_depth = z_depth + res\n",
    "        # 3.2\n",
    "        z_point = self.point_conv(z_depth) # B * D, L, P -> B * D, L, P\n",
    "        z_point = self.point_activation(z_point)\n",
    "        z_point = self.point_norm(z_point)\n",
    "        z_point = z_point.reshape(B, D, -1) # B * D, L, P -> B, D, L * P\n",
    "\n",
    "        # 4\n",
    "        z_mlp = self.mlp(z_point) # B, D, L * P -> B, D, H\n",
    "\n",
    "        return (z_res + z_mlp).permute(0,2,1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, pred_len, patch_len, enc_in, stride, padding_patch):\n",
    "        super(Model, self).__init__()\n",
    "        self.rev = RevIN(enc_in)\n",
    "\n",
    "        self.backbone = Backbone(seq_len, pred_len, patch_len, stride, padding_patch)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "\n",
    "    def forward(self, x, batch_x_mark, dec_inp, batch_y_mark):\n",
    "        z = self.rev(x, 'norm') # B, L, D -> B, L, D\n",
    "        z = self.backbone(z) # B, L, D -> B, H, D\n",
    "        z = self.rev(z, 'denorm') # B, L, D -> B, H, D\n",
    "        return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38aebd89-3c47-4ff1-9a47-0c3f3d30c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x, y):\n",
    "        return 0.5 * F.mse_loss(x, y) + 0.5 * F.l1_loss(x, y)\n",
    "\n",
    "class Exp_Main():\n",
    "    def __init__(self, learning_rate, seq_len, pred_len, patch_len, enc_in, stride, padding_patch, epochs, model_name , df):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.patch_len = patch_len\n",
    "        self.enc_in = enc_in\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        self.epochs = epochs\n",
    "        self.use_amp = False\n",
    "        self.output_attention = False\n",
    "        self.checkpoints = './checkpoints/'\n",
    "        self.lradj = 'type3'\n",
    "        self.label_len = 0\n",
    "        self.df = df\n",
    "        self.device = 'cuda'\n",
    "        self.model_name = model_name\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Model(self.seq_len, self.pred_len, self.patch_len, self.enc_in, self.stride, self.padding_patch).to(self.device).float()\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(flag= flag, df = self.df, batch_size = 256 , freq = 'd' , seq_len = self.seq_len, label_len = self.label_len\n",
    "                                              , pred_len = self.pred_len, target = 'y', num_workers = 15)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.AdamW(self.model.parameters(), lr=self.learning_rate) # for PatchMixer\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = CustomLoss() # for PatchMixer\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                \n",
    "                # encoder - decoder\n",
    "                if self.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x, batch_x_mark)\n",
    "                        else:\n",
    "                            if False:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                        outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if False: #output_attention\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1\n",
    "                outputs = outputs[:, -self.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "                total_loss.append(loss)\n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='train')\n",
    "        vali_data, vali_loader = self._get_data(flag='val')\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "        path = os.path.join('./checkpoints/', setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "\n",
    "        if False:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            \n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer = model_optim,\n",
    "                                            steps_per_epoch = train_steps,\n",
    "                                            pct_start = 0.3,\n",
    "                                            epochs = self.epochs,\n",
    "                                            max_lr = self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if self.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                        f_dim = -1\n",
    "                        outputs = outputs[:, -self.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -self.pred_len:, f_dim:].to(self.device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            \n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    # print(outputs.shape,batch_y.shape)\n",
    "                    f_dim = -1\n",
    "                    outputs = outputs[:, -self.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -self.pred_len:, f_dim:].to(self.device)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                if self.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(model_optim)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    model_optim.step()\n",
    "                    \n",
    "                if self.lradj == 'TST':\n",
    "                    adjust_learning_rate(model_optim, scheduler, epoch + 1, lradj = self.lradj, learning_rate = self.learning_rate , printout=False)\n",
    "                    scheduler.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
    "            test_loss = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if self.lradj != 'TST':\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, lradj = self.lradj, learning_rate = self.learning_rate)\n",
    "            else:\n",
    "                print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        \n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        inputx = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1\n",
    "                # print(outputs.shape,batch_y.shape)\n",
    "                outputs = outputs[:, -self.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.pred_len:, f_dim:].to(self.device)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "                pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                inputx.append(batch_x.detach().cpu().numpy())\n",
    "                if i % 20 == 0:\n",
    "                    input = batch_x.detach().cpu().numpy()\n",
    "                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "\n",
    "        if False:\n",
    "            test_params_flop((batch_x.shape[1],batch_x.shape[2]))\n",
    "            exit()\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)\n",
    "        inputx = np.array(inputx)\n",
    "\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "        print('mse:{}, mae:{}, rse:{}'.format(mse, mae, rse))\n",
    "        f = open(\"result.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('mse:{}, mae:{}, rse:{}'.format(mse, mae, rse))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "        # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n",
    "        np.save(folder_path + 'pred.npy', preds)\n",
    "        # np.save(folder_path + 'true.npy', trues)\n",
    "        # np.save(folder_path + 'x.npy', inputx)\n",
    "        return\n",
    "\n",
    "    def predict(self, setting, load=False):\n",
    "        pred_data, pred_loader = self._get_data(flag='pred')\n",
    "\n",
    "        if load:\n",
    "            path = os.path.join(self.checkpoints, setting)\n",
    "            best_model_path = path + '/' + 'checkpoint.pth'\n",
    "            self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(pred_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros([batch_y.shape[0], self.pred_len, batch_y.shape[2]]).float().to(batch_y.device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.model_name or 'TST' in self.model_name:\n",
    "                        outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                pred = outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        np.save(folder_path + 'real_prediction.npy', preds)\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a08764-d35c-4458-8e9c-786a2915c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 716\n",
    "PRED_LEN = 358\n",
    "PATCH_LEN = 1\n",
    "ENC_IN = 6\n",
    "STRIDE = 2\n",
    "PADDING_PATCH = 'end'\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "MODEL = 'PatchMixer'\n",
    "setting = 'dacon_seoul'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fce9379-6620-4a26-b7b4-0603ce385850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : dacon_seoul>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "y\n",
      "train 19636\n",
      "y\n",
      "val 795\n",
      "y\n",
      "test 793\n",
      "Epoch: 1 cost time: 1.0620646476745605\n",
      "Epoch: 1, Steps: 76 | Train Loss: 0.8091556 Vali Loss: 0.2634770 Test Loss: 0.2704520\n",
      "Validation loss decreased (inf --> 0.263477).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "Epoch: 2 cost time: 0.8964014053344727\n",
      "Epoch: 2, Steps: 76 | Train Loss: 0.4151963 Vali Loss: 0.1987759 Test Loss: 0.2099024\n",
      "Validation loss decreased (0.263477 --> 0.198776).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "Epoch: 3 cost time: 0.9655640125274658\n",
      "Epoch: 3, Steps: 76 | Train Loss: 0.2671661 Vali Loss: 0.1819944 Test Loss: 0.1932975\n",
      "Validation loss decreased (0.198776 --> 0.181994).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "Epoch: 4 cost time: 0.976546049118042\n",
      "Epoch: 4, Steps: 76 | Train Loss: 0.2268011 Vali Loss: 0.1785264 Test Loss: 0.1895675\n",
      "Validation loss decreased (0.181994 --> 0.178526).  Saving model ...\n",
      "Updating learning rate to 0.0009000000000000001\n",
      "Epoch: 5 cost time: 0.9475276470184326\n",
      "Epoch: 5, Steps: 76 | Train Loss: 0.2056454 Vali Loss: 0.1761952 Test Loss: 0.1869896\n",
      "Validation loss decreased (0.178526 --> 0.176195).  Saving model ...\n",
      "Updating learning rate to 0.0008100000000000001\n",
      "Epoch: 6 cost time: 0.9458696842193604\n",
      "Epoch: 6, Steps: 76 | Train Loss: 0.1938937 Vali Loss: 0.1767608 Test Loss: 0.1876608\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 0.0007290000000000002\n",
      "Epoch: 7 cost time: 0.9540126323699951\n",
      "Epoch: 7, Steps: 76 | Train Loss: 0.1864728 Vali Loss: 0.1730789 Test Loss: 0.1859700\n",
      "Validation loss decreased (0.176195 --> 0.173079).  Saving model ...\n",
      "Updating learning rate to 0.0006561000000000001\n",
      "Epoch: 8 cost time: 0.8958685398101807\n",
      "Epoch: 8, Steps: 76 | Train Loss: 0.1816478 Vali Loss: 0.1745306 Test Loss: 0.1873730\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 0.00059049\n",
      "Epoch: 9 cost time: 0.8710556030273438\n",
      "Epoch: 9, Steps: 76 | Train Loss: 0.1781384 Vali Loss: 0.1718692 Test Loss: 0.1849116\n",
      "Validation loss decreased (0.173079 --> 0.171869).  Saving model ...\n",
      "Updating learning rate to 0.000531441\n",
      "Epoch: 10 cost time: 0.9200711250305176\n",
      "Epoch: 10, Steps: 76 | Train Loss: 0.1758306 Vali Loss: 0.1721550 Test Loss: 0.1851665\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 0.0004782969000000001\n",
      "Epoch: 11 cost time: 0.9617948532104492\n",
      "Epoch: 11, Steps: 76 | Train Loss: 0.1743670 Vali Loss: 0.1702460 Test Loss: 0.1842940\n",
      "Validation loss decreased (0.171869 --> 0.170246).  Saving model ...\n",
      "Updating learning rate to 0.0004304672100000001\n",
      "Epoch: 12 cost time: 0.8951053619384766\n",
      "Epoch: 12, Steps: 76 | Train Loss: 0.1729455 Vali Loss: 0.1705572 Test Loss: 0.1841598\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 0.0003874204890000001\n",
      "Epoch: 13 cost time: 0.9874794483184814\n",
      "Epoch: 13, Steps: 76 | Train Loss: 0.1720974 Vali Loss: 0.1707363 Test Loss: 0.1839142\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 0.0003486784401000001\n",
      "Epoch: 14 cost time: 0.9376749992370605\n",
      "Epoch: 14, Steps: 76 | Train Loss: 0.1716016 Vali Loss: 0.1710812 Test Loss: 0.1831905\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 0.0003138105960900001\n",
      "Epoch: 15 cost time: 0.962744951248169\n",
      "Epoch: 15, Steps: 76 | Train Loss: 0.1712018 Vali Loss: 0.1708217 Test Loss: 0.1840776\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 0.0002824295364810001\n",
      "Epoch: 16 cost time: 0.9281244277954102\n",
      "Epoch: 16, Steps: 76 | Train Loss: 0.1707135 Vali Loss: 0.1716684 Test Loss: 0.1846080\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 0.0002541865828329001\n",
      "Epoch: 17 cost time: 0.8486948013305664\n",
      "Epoch: 17, Steps: 76 | Train Loss: 0.1699035 Vali Loss: 0.1706160 Test Loss: 0.1839879\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 0.0002287679245496101\n",
      "Epoch: 18 cost time: 0.8760178089141846\n",
      "Epoch: 18, Steps: 76 | Train Loss: 0.1694974 Vali Loss: 0.1694490 Test Loss: 0.1827021\n",
      "Validation loss decreased (0.170246 --> 0.169449).  Saving model ...\n",
      "Updating learning rate to 0.0002058911320946491\n",
      "Epoch: 19 cost time: 0.9976282119750977\n",
      "Epoch: 19, Steps: 76 | Train Loss: 0.1691386 Vali Loss: 0.1692517 Test Loss: 0.1824418\n",
      "Validation loss decreased (0.169449 --> 0.169252).  Saving model ...\n",
      "Updating learning rate to 0.00018530201888518417\n",
      "Epoch: 20 cost time: 0.9081013202667236\n",
      "Epoch: 20, Steps: 76 | Train Loss: 0.1691146 Vali Loss: 0.1704868 Test Loss: 0.1828728\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 0.00016677181699666576\n",
      "Epoch: 21 cost time: 0.9088499546051025\n",
      "Epoch: 21, Steps: 76 | Train Loss: 0.1687787 Vali Loss: 0.1702757 Test Loss: 0.1824166\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 0.00015009463529699917\n",
      "Epoch: 22 cost time: 0.9635906219482422\n",
      "Epoch: 22, Steps: 76 | Train Loss: 0.1684267 Vali Loss: 0.1701220 Test Loss: 0.1829171\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 0.0001350851717672993\n",
      "Epoch: 23 cost time: 0.9377293586730957\n",
      "Epoch: 23, Steps: 76 | Train Loss: 0.1684346 Vali Loss: 0.1693046 Test Loss: 0.1827643\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 0.00012157665459056935\n",
      "Epoch: 24 cost time: 0.9224894046783447\n",
      "Epoch: 24, Steps: 76 | Train Loss: 0.1682815 Vali Loss: 0.1697660 Test Loss: 0.1834899\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 0.00010941898913151242\n",
      "Epoch: 25 cost time: 0.9276878833770752\n",
      "Epoch: 25, Steps: 76 | Train Loss: 0.1682311 Vali Loss: 0.1699642 Test Loss: 0.1819376\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 9.847709021836118e-05\n",
      "Epoch: 26 cost time: 0.8939244747161865\n",
      "Epoch: 26, Steps: 76 | Train Loss: 0.1680135 Vali Loss: 0.1692758 Test Loss: 0.1827786\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 8.862938119652506e-05\n",
      "Epoch: 27 cost time: 0.9151532649993896\n",
      "Epoch: 27, Steps: 76 | Train Loss: 0.1676459 Vali Loss: 0.1697126 Test Loss: 0.1828576\n",
      "EarlyStopping counter: 8 out of 50\n",
      "Updating learning rate to 7.976644307687256e-05\n",
      "Epoch: 28 cost time: 0.8915152549743652\n",
      "Epoch: 28, Steps: 76 | Train Loss: 0.1675166 Vali Loss: 0.1695915 Test Loss: 0.1826071\n",
      "EarlyStopping counter: 9 out of 50\n",
      "Updating learning rate to 7.17897987691853e-05\n",
      "Epoch: 29 cost time: 0.9413154125213623\n",
      "Epoch: 29, Steps: 76 | Train Loss: 0.1675068 Vali Loss: 0.1698172 Test Loss: 0.1820977\n",
      "EarlyStopping counter: 10 out of 50\n",
      "Updating learning rate to 6.461081889226677e-05\n",
      "Epoch: 30 cost time: 0.978344202041626\n",
      "Epoch: 30, Steps: 76 | Train Loss: 0.1674989 Vali Loss: 0.1692231 Test Loss: 0.1828415\n",
      "Validation loss decreased (0.169252 --> 0.169223).  Saving model ...\n",
      "Updating learning rate to 5.8149737003040094e-05\n",
      "Epoch: 31 cost time: 0.8518431186676025\n",
      "Epoch: 31, Steps: 76 | Train Loss: 0.1673210 Vali Loss: 0.1693741 Test Loss: 0.1823835\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 5.233476330273609e-05\n",
      "Epoch: 32 cost time: 0.933349609375\n",
      "Epoch: 32, Steps: 76 | Train Loss: 0.1673082 Vali Loss: 0.1691162 Test Loss: 0.1826766\n",
      "Validation loss decreased (0.169223 --> 0.169116).  Saving model ...\n",
      "Updating learning rate to 4.7101286972462485e-05\n",
      "Epoch: 33 cost time: 0.9424688816070557\n",
      "Epoch: 33, Steps: 76 | Train Loss: 0.1670978 Vali Loss: 0.1692456 Test Loss: 0.1821905\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 4.239115827521624e-05\n",
      "Epoch: 34 cost time: 0.9414305686950684\n",
      "Epoch: 34, Steps: 76 | Train Loss: 0.1673513 Vali Loss: 0.1692427 Test Loss: 0.1823933\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 3.8152042447694614e-05\n",
      "Epoch: 35 cost time: 0.9304840564727783\n",
      "Epoch: 35, Steps: 76 | Train Loss: 0.1671763 Vali Loss: 0.1692938 Test Loss: 0.1821281\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 3.433683820292515e-05\n",
      "Epoch: 36 cost time: 0.8857762813568115\n",
      "Epoch: 36, Steps: 76 | Train Loss: 0.1669869 Vali Loss: 0.1692482 Test Loss: 0.1827059\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 3.090315438263264e-05\n",
      "Epoch: 37 cost time: 0.8865394592285156\n",
      "Epoch: 37, Steps: 76 | Train Loss: 0.1669621 Vali Loss: 0.1697994 Test Loss: 0.1825641\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 2.7812838944369376e-05\n",
      "Epoch: 38 cost time: 0.9260015487670898\n",
      "Epoch: 38, Steps: 76 | Train Loss: 0.1670345 Vali Loss: 0.1695836 Test Loss: 0.1823546\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 2.5031555049932436e-05\n",
      "Epoch: 39 cost time: 0.9454250335693359\n",
      "Epoch: 39, Steps: 76 | Train Loss: 0.1670156 Vali Loss: 0.1693819 Test Loss: 0.1825756\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 2.2528399544939195e-05\n",
      "Epoch: 40 cost time: 0.9143650531768799\n",
      "Epoch: 40, Steps: 76 | Train Loss: 0.1669544 Vali Loss: 0.1689651 Test Loss: 0.1824052\n",
      "Validation loss decreased (0.169116 --> 0.168965).  Saving model ...\n",
      "Updating learning rate to 2.0275559590445276e-05\n",
      "Epoch: 41 cost time: 0.995002269744873\n",
      "Epoch: 41, Steps: 76 | Train Loss: 0.1669147 Vali Loss: 0.1689401 Test Loss: 0.1822528\n",
      "Validation loss decreased (0.168965 --> 0.168940).  Saving model ...\n",
      "Updating learning rate to 1.824800363140075e-05\n",
      "Epoch: 42 cost time: 0.9661037921905518\n",
      "Epoch: 42, Steps: 76 | Train Loss: 0.1669935 Vali Loss: 0.1693970 Test Loss: 0.1824056\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 1.6423203268260675e-05\n",
      "Epoch: 43 cost time: 0.9887692928314209\n",
      "Epoch: 43, Steps: 76 | Train Loss: 0.1668283 Vali Loss: 0.1692539 Test Loss: 0.1822143\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 1.4780882941434608e-05\n",
      "Epoch: 44 cost time: 0.9211158752441406\n",
      "Epoch: 44, Steps: 76 | Train Loss: 0.1668447 Vali Loss: 0.1693505 Test Loss: 0.1825855\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 1.3302794647291146e-05\n",
      "Epoch: 45 cost time: 0.8755064010620117\n",
      "Epoch: 45, Steps: 76 | Train Loss: 0.1668894 Vali Loss: 0.1691005 Test Loss: 0.1823732\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 1.1972515182562033e-05\n",
      "Epoch: 46 cost time: 0.9516942501068115\n",
      "Epoch: 46, Steps: 76 | Train Loss: 0.1666671 Vali Loss: 0.1690044 Test Loss: 0.1823901\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 1.077526366430583e-05\n",
      "Epoch: 47 cost time: 0.9679570198059082\n",
      "Epoch: 47, Steps: 76 | Train Loss: 0.1667652 Vali Loss: 0.1691030 Test Loss: 0.1824446\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 9.697737297875246e-06\n",
      "Epoch: 48 cost time: 0.9627246856689453\n",
      "Epoch: 48, Steps: 76 | Train Loss: 0.1667421 Vali Loss: 0.1693382 Test Loss: 0.1823875\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 8.727963568087722e-06\n",
      "Epoch: 49 cost time: 0.9812047481536865\n",
      "Epoch: 49, Steps: 76 | Train Loss: 0.1667227 Vali Loss: 0.1693822 Test Loss: 0.1821171\n",
      "EarlyStopping counter: 8 out of 50\n",
      "Updating learning rate to 7.85516721127895e-06\n",
      "Epoch: 50 cost time: 0.9559452533721924\n",
      "Epoch: 50, Steps: 76 | Train Loss: 0.1666093 Vali Loss: 0.1689231 Test Loss: 0.1822577\n",
      "Validation loss decreased (0.168940 --> 0.168923).  Saving model ...\n",
      "Updating learning rate to 7.069650490151056e-06\n",
      "Epoch: 51 cost time: 0.8905754089355469\n",
      "Epoch: 51, Steps: 76 | Train Loss: 0.1665266 Vali Loss: 0.1693438 Test Loss: 0.1823777\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 6.36268544113595e-06\n",
      "Epoch: 52 cost time: 0.9051239490509033\n",
      "Epoch: 52, Steps: 76 | Train Loss: 0.1667293 Vali Loss: 0.1689975 Test Loss: 0.1822203\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 5.726416897022355e-06\n",
      "Epoch: 53 cost time: 0.8999495506286621\n",
      "Epoch: 53, Steps: 76 | Train Loss: 0.1664485 Vali Loss: 0.1693545 Test Loss: 0.1824053\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 5.15377520732012e-06\n",
      "Epoch: 54 cost time: 0.935821533203125\n",
      "Epoch: 54, Steps: 76 | Train Loss: 0.1667349 Vali Loss: 0.1693168 Test Loss: 0.1823448\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 4.638397686588108e-06\n",
      "Epoch: 55 cost time: 0.8966741561889648\n",
      "Epoch: 55, Steps: 76 | Train Loss: 0.1666218 Vali Loss: 0.1693472 Test Loss: 0.1822162\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 4.174557917929297e-06\n",
      "Epoch: 56 cost time: 0.8778839111328125\n",
      "Epoch: 56, Steps: 76 | Train Loss: 0.1667241 Vali Loss: 0.1693460 Test Loss: 0.1821939\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 3.7571021261363675e-06\n",
      "Epoch: 57 cost time: 1.0047454833984375\n",
      "Epoch: 57, Steps: 76 | Train Loss: 0.1666023 Vali Loss: 0.1692088 Test Loss: 0.1823764\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 3.381391913522731e-06\n",
      "Epoch: 58 cost time: 0.9459409713745117\n",
      "Epoch: 58, Steps: 76 | Train Loss: 0.1666550 Vali Loss: 0.1690341 Test Loss: 0.1822998\n",
      "EarlyStopping counter: 8 out of 50\n",
      "Updating learning rate to 3.0432527221704577e-06\n",
      "Epoch: 59 cost time: 0.9372599124908447\n",
      "Epoch: 59, Steps: 76 | Train Loss: 0.1666002 Vali Loss: 0.1691879 Test Loss: 0.1823641\n",
      "EarlyStopping counter: 9 out of 50\n",
      "Updating learning rate to 2.7389274499534123e-06\n",
      "Epoch: 60 cost time: 0.916118860244751\n",
      "Epoch: 60, Steps: 76 | Train Loss: 0.1665421 Vali Loss: 0.1689337 Test Loss: 0.1822284\n",
      "EarlyStopping counter: 10 out of 50\n",
      "Updating learning rate to 2.465034704958071e-06\n",
      "Epoch: 61 cost time: 0.8998420238494873\n",
      "Epoch: 61, Steps: 76 | Train Loss: 0.1665428 Vali Loss: 0.1693261 Test Loss: 0.1823591\n",
      "EarlyStopping counter: 11 out of 50\n",
      "Updating learning rate to 2.218531234462264e-06\n",
      "Epoch: 62 cost time: 0.9000792503356934\n",
      "Epoch: 62, Steps: 76 | Train Loss: 0.1666266 Vali Loss: 0.1693436 Test Loss: 0.1822764\n",
      "EarlyStopping counter: 12 out of 50\n",
      "Updating learning rate to 1.9966781110160375e-06\n",
      "Epoch: 63 cost time: 0.9526264667510986\n",
      "Epoch: 63, Steps: 76 | Train Loss: 0.1665604 Vali Loss: 0.1690688 Test Loss: 0.1822917\n",
      "EarlyStopping counter: 13 out of 50\n",
      "Updating learning rate to 1.797010299914434e-06\n",
      "Epoch: 64 cost time: 0.8924651145935059\n",
      "Epoch: 64, Steps: 76 | Train Loss: 0.1664630 Vali Loss: 0.1689605 Test Loss: 0.1822576\n",
      "EarlyStopping counter: 14 out of 50\n",
      "Updating learning rate to 1.6173092699229906e-06\n",
      "Epoch: 65 cost time: 0.8707320690155029\n",
      "Epoch: 65, Steps: 76 | Train Loss: 0.1666954 Vali Loss: 0.1690877 Test Loss: 0.1822825\n",
      "EarlyStopping counter: 15 out of 50\n",
      "Updating learning rate to 1.4555783429306915e-06\n",
      "Epoch: 66 cost time: 0.9396953582763672\n",
      "Epoch: 66, Steps: 76 | Train Loss: 0.1665167 Vali Loss: 0.1690956 Test Loss: 0.1823118\n",
      "EarlyStopping counter: 16 out of 50\n",
      "Updating learning rate to 1.3100205086376223e-06\n",
      "Epoch: 67 cost time: 0.9123470783233643\n",
      "Epoch: 67, Steps: 76 | Train Loss: 0.1664724 Vali Loss: 0.1689501 Test Loss: 0.1822478\n",
      "EarlyStopping counter: 17 out of 50\n",
      "Updating learning rate to 1.1790184577738603e-06\n",
      "Epoch: 68 cost time: 0.8859367370605469\n",
      "Epoch: 68, Steps: 76 | Train Loss: 0.1666304 Vali Loss: 0.1690488 Test Loss: 0.1822890\n",
      "EarlyStopping counter: 18 out of 50\n",
      "Updating learning rate to 1.061116611996474e-06\n",
      "Epoch: 69 cost time: 0.8883683681488037\n",
      "Epoch: 69, Steps: 76 | Train Loss: 0.1665442 Vali Loss: 0.1692687 Test Loss: 0.1822699\n",
      "EarlyStopping counter: 19 out of 50\n",
      "Updating learning rate to 9.550049507968267e-07\n",
      "Epoch: 70 cost time: 0.9184455871582031\n",
      "Epoch: 70, Steps: 76 | Train Loss: 0.1665539 Vali Loss: 0.1691291 Test Loss: 0.1822851\n",
      "EarlyStopping counter: 20 out of 50\n",
      "Updating learning rate to 8.595044557171441e-07\n",
      "Epoch: 71 cost time: 0.9176080226898193\n",
      "Epoch: 71, Steps: 76 | Train Loss: 0.1665496 Vali Loss: 0.1689891 Test Loss: 0.1824506\n",
      "EarlyStopping counter: 21 out of 50\n",
      "Updating learning rate to 7.735540101454298e-07\n",
      "Epoch: 72 cost time: 0.9664723873138428\n",
      "Epoch: 72, Steps: 76 | Train Loss: 0.1664073 Vali Loss: 0.1688572 Test Loss: 0.1823810\n",
      "Validation loss decreased (0.168923 --> 0.168857).  Saving model ...\n",
      "Updating learning rate to 6.961986091308867e-07\n",
      "Epoch: 73 cost time: 0.877526044845581\n",
      "Epoch: 73, Steps: 76 | Train Loss: 0.1665900 Vali Loss: 0.1692762 Test Loss: 0.1823124\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 6.265787482177981e-07\n",
      "Epoch: 74 cost time: 0.8738129138946533\n",
      "Epoch: 74, Steps: 76 | Train Loss: 0.1664335 Vali Loss: 0.1694282 Test Loss: 0.1823912\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 5.639208733960184e-07\n",
      "Epoch: 75 cost time: 0.9407217502593994\n",
      "Epoch: 75, Steps: 76 | Train Loss: 0.1665193 Vali Loss: 0.1688998 Test Loss: 0.1822726\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 5.075287860564166e-07\n",
      "Epoch: 76 cost time: 0.9649395942687988\n",
      "Epoch: 76, Steps: 76 | Train Loss: 0.1665371 Vali Loss: 0.1690460 Test Loss: 0.1822008\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 4.5677590745077486e-07\n",
      "Epoch: 77 cost time: 0.9276306629180908\n",
      "Epoch: 77, Steps: 76 | Train Loss: 0.1665997 Vali Loss: 0.1692672 Test Loss: 0.1822331\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 4.110983167056974e-07\n",
      "Epoch: 78 cost time: 0.9370584487915039\n",
      "Epoch: 78, Steps: 76 | Train Loss: 0.1665511 Vali Loss: 0.1691600 Test Loss: 0.1822797\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 3.699884850351277e-07\n",
      "Epoch: 79 cost time: 0.8952498435974121\n",
      "Epoch: 79, Steps: 76 | Train Loss: 0.1664496 Vali Loss: 0.1691883 Test Loss: 0.1823228\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 3.329896365316149e-07\n",
      "Epoch: 80 cost time: 0.9068808555603027\n",
      "Epoch: 80, Steps: 76 | Train Loss: 0.1666033 Vali Loss: 0.1691063 Test Loss: 0.1822522\n",
      "EarlyStopping counter: 8 out of 50\n",
      "Updating learning rate to 2.996906728784534e-07\n",
      "Epoch: 81 cost time: 0.8789803981781006\n",
      "Epoch: 81, Steps: 76 | Train Loss: 0.1665792 Vali Loss: 0.1689759 Test Loss: 0.1822672\n",
      "EarlyStopping counter: 9 out of 50\n",
      "Updating learning rate to 2.6972160559060813e-07\n",
      "Epoch: 82 cost time: 0.9262113571166992\n",
      "Epoch: 82, Steps: 76 | Train Loss: 0.1664935 Vali Loss: 0.1689310 Test Loss: 0.1822588\n",
      "EarlyStopping counter: 10 out of 50\n",
      "Updating learning rate to 2.427494450315473e-07\n",
      "Epoch: 83 cost time: 0.8876521587371826\n",
      "Epoch: 83, Steps: 76 | Train Loss: 0.1664960 Vali Loss: 0.1689279 Test Loss: 0.1822153\n",
      "EarlyStopping counter: 11 out of 50\n",
      "Updating learning rate to 2.1847450052839256e-07\n",
      "Epoch: 84 cost time: 0.9544715881347656\n",
      "Epoch: 84, Steps: 76 | Train Loss: 0.1665373 Vali Loss: 0.1689663 Test Loss: 0.1822697\n",
      "EarlyStopping counter: 12 out of 50\n",
      "Updating learning rate to 1.966270504755533e-07\n",
      "Epoch: 85 cost time: 0.9812037944793701\n",
      "Epoch: 85, Steps: 76 | Train Loss: 0.1664082 Vali Loss: 0.1690263 Test Loss: 0.1822170\n",
      "EarlyStopping counter: 13 out of 50\n",
      "Updating learning rate to 1.7696434542799797e-07\n",
      "Epoch: 86 cost time: 0.8609399795532227\n",
      "Epoch: 86, Steps: 76 | Train Loss: 0.1664939 Vali Loss: 0.1692369 Test Loss: 0.1824806\n",
      "EarlyStopping counter: 14 out of 50\n",
      "Updating learning rate to 1.5926791088519817e-07\n",
      "Epoch: 87 cost time: 0.9793233871459961\n",
      "Epoch: 87, Steps: 76 | Train Loss: 0.1666387 Vali Loss: 0.1690677 Test Loss: 0.1822265\n",
      "EarlyStopping counter: 15 out of 50\n",
      "Updating learning rate to 1.4334111979667837e-07\n",
      "Epoch: 88 cost time: 0.9458093643188477\n",
      "Epoch: 88, Steps: 76 | Train Loss: 0.1666079 Vali Loss: 0.1690792 Test Loss: 0.1822621\n",
      "EarlyStopping counter: 16 out of 50\n",
      "Updating learning rate to 1.2900700781701054e-07\n",
      "Epoch: 89 cost time: 0.8561389446258545\n",
      "Epoch: 89, Steps: 76 | Train Loss: 0.1664574 Vali Loss: 0.1690857 Test Loss: 0.1822707\n",
      "EarlyStopping counter: 17 out of 50\n",
      "Updating learning rate to 1.1610630703530949e-07\n",
      "Epoch: 90 cost time: 0.8825488090515137\n",
      "Epoch: 90, Steps: 76 | Train Loss: 0.1664314 Vali Loss: 0.1691210 Test Loss: 0.1823145\n",
      "EarlyStopping counter: 18 out of 50\n",
      "Updating learning rate to 1.0449567633177854e-07\n",
      "Epoch: 91 cost time: 0.8901913166046143\n",
      "Epoch: 91, Steps: 76 | Train Loss: 0.1665789 Vali Loss: 0.1689680 Test Loss: 0.1822567\n",
      "EarlyStopping counter: 19 out of 50\n",
      "Updating learning rate to 9.404610869860068e-08\n",
      "Epoch: 92 cost time: 0.9593729972839355\n",
      "Epoch: 92, Steps: 76 | Train Loss: 0.1665329 Vali Loss: 0.1689262 Test Loss: 0.1822504\n",
      "EarlyStopping counter: 20 out of 50\n",
      "Updating learning rate to 8.464149782874062e-08\n",
      "Epoch: 93 cost time: 0.9783172607421875\n",
      "Epoch: 93, Steps: 76 | Train Loss: 0.1666134 Vali Loss: 0.1688560 Test Loss: 0.1822062\n",
      "Validation loss decreased (0.168857 --> 0.168856).  Saving model ...\n",
      "Updating learning rate to 7.617734804586657e-08\n",
      "Epoch: 94 cost time: 0.8397152423858643\n",
      "Epoch: 94, Steps: 76 | Train Loss: 0.1666946 Vali Loss: 0.1690451 Test Loss: 0.1822581\n",
      "EarlyStopping counter: 1 out of 50\n",
      "Updating learning rate to 6.85596132412799e-08\n",
      "Epoch: 95 cost time: 0.947685718536377\n",
      "Epoch: 95, Steps: 76 | Train Loss: 0.1665593 Vali Loss: 0.1692942 Test Loss: 0.1823081\n",
      "EarlyStopping counter: 2 out of 50\n",
      "Updating learning rate to 6.170365191715192e-08\n",
      "Epoch: 96 cost time: 0.869765043258667\n",
      "Epoch: 96, Steps: 76 | Train Loss: 0.1665994 Vali Loss: 0.1690369 Test Loss: 0.1823345\n",
      "EarlyStopping counter: 3 out of 50\n",
      "Updating learning rate to 5.5533286725436726e-08\n",
      "Epoch: 97 cost time: 0.8603343963623047\n",
      "Epoch: 97, Steps: 76 | Train Loss: 0.1665280 Vali Loss: 0.1690477 Test Loss: 0.1822528\n",
      "EarlyStopping counter: 4 out of 50\n",
      "Updating learning rate to 4.997995805289306e-08\n",
      "Epoch: 98 cost time: 1.0738577842712402\n",
      "Epoch: 98, Steps: 76 | Train Loss: 0.1664831 Vali Loss: 0.1689513 Test Loss: 0.1822039\n",
      "EarlyStopping counter: 5 out of 50\n",
      "Updating learning rate to 4.498196224760375e-08\n",
      "Epoch: 99 cost time: 0.9927029609680176\n",
      "Epoch: 99, Steps: 76 | Train Loss: 0.1665119 Vali Loss: 0.1692371 Test Loss: 0.1823655\n",
      "EarlyStopping counter: 6 out of 50\n",
      "Updating learning rate to 4.048376602284338e-08\n",
      "Epoch: 100 cost time: 0.914658784866333\n",
      "Epoch: 100, Steps: 76 | Train Loss: 0.1666178 Vali Loss: 0.1691494 Test Loss: 0.1822548\n",
      "EarlyStopping counter: 7 out of 50\n",
      "Updating learning rate to 3.643538942055904e-08\n",
      ">>>>>>>testing : dacon_seoul<.<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "y\n",
      "test 793\n",
      "mse:0.10969898849725723, mae:0.2547132968902588, rse:0.3328990042209625\n",
      ">>>>>>>predicting : dacon_seoul<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp = Exp_Main(learning_rate = LEARNING_RATE, seq_len = SEQ_LEN, pred_len = PRED_LEN, \n",
    "               patch_len = PATCH_LEN, enc_in = ENC_IN, stride = STRIDE, padding_patch = PADDING_PATCH, \n",
    "               epochs = EPOCHS, model_name = MODEL , df = df_train)\n",
    "\n",
    "print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "exp.train(setting)\n",
    "print('>>>>>>>testing : {}<.<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "exp.test(setting)\n",
    "print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "exp.predict(setting, True)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a3f379-8435-4003-975b-11d39b4204bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchMixer_pred = np.load('./results/dacon_seoul/real_prediction.npy')\n",
    "\n",
    "train_mean = df_train.mean()\n",
    "train_std = df_train.std()\n",
    "\n",
    "PatchMixer_pred = PatchMixer_pred.reshape(PRED_LEN,ENC_IN)\n",
    "df_PatchMixer_pred = pd.DataFrame(PatchMixer_pred)\n",
    "df_PatchMixer_pred = df_PatchMixer_pred*train_std['y'] + train_mean['y']\n",
    "pr = df_PatchMixer_pred[ENC_IN-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72623c27-c131-4649-ad1f-efcab2b63308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prophet = df_train.copy()\n",
    "\n",
    "# df_prophet.rename(columns={'date':'ds'}, inplace=True)\n",
    "\n",
    "# prophet_model = Prophet(growth = 'linear',\n",
    "#                 changepoint_prior_scale = 0.8,\n",
    "#                 changepoint_range = 0.8,\n",
    "#                 seasonality_prior_scale = 12.0,\n",
    "#                 yearly_seasonality = 15,\n",
    "#                 n_changepoints = 744,\n",
    "#                 seasonality_mode = 'additive').add_seasonality(name = 'season', period = 90, fourier_order=6)\n",
    "# prophet_model.fit(df_prophet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cce66d-dea8-4169-8d36-3cbf575ec359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_data = prophet_model.make_future_dataframe(periods = PRED_LEN, freq = 'd')\n",
    "# forecast_data = prophet_model.predict(future_data)\n",
    "# fr = forecast_data.yhat[-PRED_LEN:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae392eb-a948-4c6a-a005-daa4d5f16adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft_voting = fr * 0.5 + pr * 0.5\n",
    "#soft_voting = fr\n",
    "\n",
    "soft_voting = pr\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sub['평균기온'] = soft_voting\n",
    "sub.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
